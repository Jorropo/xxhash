//go:build go1.17
// +build go1.17

package xxhash

// Generated by gen/slide.go. DO NOT EDIT.

const slideLength = 127

// Handles length 0-127 bytes using sum slides.
func slide(b []byte) uint64 {
	// This function use sum slides, theses are straight unrolled pieces of code which compute hashes, with middle jumps.
	// Each do not contain any conditions to make them trivial for the CPU to parse and never cause any pipeline flushes after the first jump table.
	// We need 32 different slides to cover each offset into the 32 block size. The trailing 32 bytes are handled by their own slides which are shared and reused by the higher slides.
	// The trailing 32 bytes slides are reused for each offset. The CPUs we care about can always correctly read unconditional jumps without causing a pipeline flush.

	// This function is written more like an optimized assembly routine, except we trick the compiler into generating good code by generating the slide ourself.
	// Using the go compiler make the call overhead cheaper since it will use the unstable ABIInternal passing through registers.
	// They are also extremely effective when hashing multiple values of the same size back to back.
	// Assumptions of this strategy:
	// - All the state except b's array will be correctly register allocated.
	//   It probably generate unnecessary MOVs but the critical path includes LAT3 multiplies for each block, so there is plenty of time to dispatch renames.
	// - The compiler is basic block based and will do a good enough job at layout. This is true for some the go compiler, llvm and some of gcc.
	//   This means I make very liberal use of goto, they shouldn't be red as JMPs but abstract basic blocks links.
	// - The compiler has some SSA passes.
	//   This is used for all the b_* tricks.

	// Setup variables here since go doesn't want use to do dangerous gotos.
	v1 := prime1
	v1 += prime2
	v2 := prime2
	v3 := uint64(0)
	v4 := prime1
	v4 = -v4
	h := prime5
	n := uint64(len(b))

	// The go compiler has multiple oversight in the propragation of proofs through Phi nodes. Using array pointers is a very unsubtle hint and compiles to nothing.
	// Because we assume the compiler has some low level SSA passes this is otherwise free.
	var (
		b_0 *[0]byte
		b_1 *[1]byte
		b_2 *[2]byte
		b_3 *[3]byte
		b_4 *[4]byte
		b_5 *[5]byte
		b_6 *[6]byte
		b_7 *[7]byte
		b_8 *[8]byte
		b_9 *[9]byte
		b_10 *[10]byte
		b_11 *[11]byte
		b_12 *[12]byte
		b_13 *[13]byte
		b_14 *[14]byte
		b_15 *[15]byte
		b_16 *[16]byte
		b_17 *[17]byte
		b_18 *[18]byte
		b_19 *[19]byte
		b_20 *[20]byte
		b_21 *[21]byte
		b_22 *[22]byte
		b_23 *[23]byte
		b_24 *[24]byte
		b_25 *[25]byte
		b_26 *[26]byte
		b_27 *[27]byte
		b_28 *[28]byte
		b_29 *[29]byte
		b_30 *[30]byte
		b_31 *[31]byte
		b_32 *[32]byte
		b_33 *[33]byte
		b_34 *[34]byte
		b_35 *[35]byte
		b_36 *[36]byte
		b_37 *[37]byte
		b_38 *[38]byte
		b_39 *[39]byte
		b_40 *[40]byte
		b_41 *[41]byte
		b_42 *[42]byte
		b_43 *[43]byte
		b_44 *[44]byte
		b_45 *[45]byte
		b_46 *[46]byte
		b_47 *[47]byte
		b_48 *[48]byte
		b_49 *[49]byte
		b_50 *[50]byte
		b_51 *[51]byte
		b_52 *[52]byte
		b_53 *[53]byte
		b_54 *[54]byte
		b_55 *[55]byte
		b_56 *[56]byte
		b_57 *[57]byte
		b_58 *[58]byte
		b_59 *[59]byte
		b_60 *[60]byte
		b_61 *[61]byte
		b_62 *[62]byte
		b_63 *[63]byte
		b_64 *[64]byte
		b_65 *[65]byte
		b_66 *[66]byte
		b_67 *[67]byte
		b_68 *[68]byte
		b_69 *[69]byte
		b_70 *[70]byte
		b_71 *[71]byte
		b_72 *[72]byte
		b_73 *[73]byte
		b_74 *[74]byte
		b_75 *[75]byte
		b_76 *[76]byte
		b_77 *[77]byte
		b_78 *[78]byte
		b_79 *[79]byte
		b_80 *[80]byte
		b_81 *[81]byte
		b_82 *[82]byte
		b_83 *[83]byte
		b_84 *[84]byte
		b_85 *[85]byte
		b_86 *[86]byte
		b_87 *[87]byte
		b_88 *[88]byte
		b_89 *[89]byte
		b_90 *[90]byte
		b_91 *[91]byte
		b_92 *[92]byte
		b_93 *[93]byte
		b_94 *[94]byte
		b_95 *[95]byte
		b_96 *[96]byte
		b_97 *[97]byte
		b_98 *[98]byte
		b_99 *[99]byte
		b_100 *[100]byte
		b_101 *[101]byte
		b_102 *[102]byte
		b_103 *[103]byte
		b_104 *[104]byte
		b_105 *[105]byte
		b_106 *[106]byte
		b_107 *[107]byte
		b_108 *[108]byte
		b_109 *[109]byte
		b_110 *[110]byte
		b_111 *[111]byte
		b_112 *[112]byte
		b_113 *[113]byte
		b_114 *[114]byte
		b_115 *[115]byte
		b_116 *[116]byte
		b_117 *[117]byte
		b_118 *[118]byte
		b_119 *[119]byte
		b_120 *[120]byte
		b_121 *[121]byte
		b_122 *[122]byte
		b_123 *[123]byte
		b_124 *[124]byte
		b_125 *[125]byte
		b_126 *[126]byte
		b_127 *[127]byte
	)

	// Jump table to various positions in the slide, this setups proofs for bounds checks.
	// From then on it need to make sure to maintain constance in the length of b.
	switch len(b) {
	case 0:
		// Handle this appart because it can be completely folded.
		h += n
		h ^= h >> 33
		h *= prime2
		h ^= h >> 29
		h *= prime3
		h ^= h >> 32
		return h
	case 1:
		b_1 = (*[1]byte)(b)
		goto sz_1
	case 2:
		b_2 = (*[2]byte)(b)
		goto sz_2
	case 3:
		b_3 = (*[3]byte)(b)
		goto sz_3
	case 4:
		b_4 = (*[4]byte)(b)
		goto sz_4
	case 5:
		b_5 = (*[5]byte)(b)
		goto sz_5
	case 6:
		b_6 = (*[6]byte)(b)
		goto sz_6
	case 7:
		b_7 = (*[7]byte)(b)
		goto sz_7
	case 8:
		b_8 = (*[8]byte)(b)
		goto sz_8
	case 9:
		b_9 = (*[9]byte)(b)
		goto sz_9
	case 10:
		b_10 = (*[10]byte)(b)
		goto sz_10
	case 11:
		b_11 = (*[11]byte)(b)
		goto sz_11
	case 12:
		b_12 = (*[12]byte)(b)
		goto sz_12
	case 13:
		b_13 = (*[13]byte)(b)
		goto sz_13
	case 14:
		b_14 = (*[14]byte)(b)
		goto sz_14
	case 15:
		b_15 = (*[15]byte)(b)
		goto sz_15
	case 16:
		b_16 = (*[16]byte)(b)
		goto sz_16
	case 17:
		b_17 = (*[17]byte)(b)
		goto sz_17
	case 18:
		b_18 = (*[18]byte)(b)
		goto sz_18
	case 19:
		b_19 = (*[19]byte)(b)
		goto sz_19
	case 20:
		b_20 = (*[20]byte)(b)
		goto sz_20
	case 21:
		b_21 = (*[21]byte)(b)
		goto sz_21
	case 22:
		b_22 = (*[22]byte)(b)
		goto sz_22
	case 23:
		b_23 = (*[23]byte)(b)
		goto sz_23
	case 24:
		b_24 = (*[24]byte)(b)
		goto sz_24
	case 25:
		b_25 = (*[25]byte)(b)
		goto sz_25
	case 26:
		b_26 = (*[26]byte)(b)
		goto sz_26
	case 27:
		b_27 = (*[27]byte)(b)
		goto sz_27
	case 28:
		b_28 = (*[28]byte)(b)
		goto sz_28
	case 29:
		b_29 = (*[29]byte)(b)
		goto sz_29
	case 30:
		b_30 = (*[30]byte)(b)
		goto sz_30
	case 31:
		b_31 = (*[31]byte)(b)
		goto sz_31
	case 32:
		b_32 = (*[32]byte)(b)
		goto sz_32
	case 33:
		b_33 = (*[33]byte)(b)
		goto sz_33
	case 34:
		b_34 = (*[34]byte)(b)
		goto sz_34
	case 35:
		b_35 = (*[35]byte)(b)
		goto sz_35
	case 36:
		b_36 = (*[36]byte)(b)
		goto sz_36
	case 37:
		b_37 = (*[37]byte)(b)
		goto sz_37
	case 38:
		b_38 = (*[38]byte)(b)
		goto sz_38
	case 39:
		b_39 = (*[39]byte)(b)
		goto sz_39
	case 40:
		b_40 = (*[40]byte)(b)
		goto sz_40
	case 41:
		b_41 = (*[41]byte)(b)
		goto sz_41
	case 42:
		b_42 = (*[42]byte)(b)
		goto sz_42
	case 43:
		b_43 = (*[43]byte)(b)
		goto sz_43
	case 44:
		b_44 = (*[44]byte)(b)
		goto sz_44
	case 45:
		b_45 = (*[45]byte)(b)
		goto sz_45
	case 46:
		b_46 = (*[46]byte)(b)
		goto sz_46
	case 47:
		b_47 = (*[47]byte)(b)
		goto sz_47
	case 48:
		b_48 = (*[48]byte)(b)
		goto sz_48
	case 49:
		b_49 = (*[49]byte)(b)
		goto sz_49
	case 50:
		b_50 = (*[50]byte)(b)
		goto sz_50
	case 51:
		b_51 = (*[51]byte)(b)
		goto sz_51
	case 52:
		b_52 = (*[52]byte)(b)
		goto sz_52
	case 53:
		b_53 = (*[53]byte)(b)
		goto sz_53
	case 54:
		b_54 = (*[54]byte)(b)
		goto sz_54
	case 55:
		b_55 = (*[55]byte)(b)
		goto sz_55
	case 56:
		b_56 = (*[56]byte)(b)
		goto sz_56
	case 57:
		b_57 = (*[57]byte)(b)
		goto sz_57
	case 58:
		b_58 = (*[58]byte)(b)
		goto sz_58
	case 59:
		b_59 = (*[59]byte)(b)
		goto sz_59
	case 60:
		b_60 = (*[60]byte)(b)
		goto sz_60
	case 61:
		b_61 = (*[61]byte)(b)
		goto sz_61
	case 62:
		b_62 = (*[62]byte)(b)
		goto sz_62
	case 63:
		b_63 = (*[63]byte)(b)
		goto sz_63
	case 64:
		b_64 = (*[64]byte)(b)
		goto sz_64
	case 65:
		b_65 = (*[65]byte)(b)
		goto sz_65
	case 66:
		b_66 = (*[66]byte)(b)
		goto sz_66
	case 67:
		b_67 = (*[67]byte)(b)
		goto sz_67
	case 68:
		b_68 = (*[68]byte)(b)
		goto sz_68
	case 69:
		b_69 = (*[69]byte)(b)
		goto sz_69
	case 70:
		b_70 = (*[70]byte)(b)
		goto sz_70
	case 71:
		b_71 = (*[71]byte)(b)
		goto sz_71
	case 72:
		b_72 = (*[72]byte)(b)
		goto sz_72
	case 73:
		b_73 = (*[73]byte)(b)
		goto sz_73
	case 74:
		b_74 = (*[74]byte)(b)
		goto sz_74
	case 75:
		b_75 = (*[75]byte)(b)
		goto sz_75
	case 76:
		b_76 = (*[76]byte)(b)
		goto sz_76
	case 77:
		b_77 = (*[77]byte)(b)
		goto sz_77
	case 78:
		b_78 = (*[78]byte)(b)
		goto sz_78
	case 79:
		b_79 = (*[79]byte)(b)
		goto sz_79
	case 80:
		b_80 = (*[80]byte)(b)
		goto sz_80
	case 81:
		b_81 = (*[81]byte)(b)
		goto sz_81
	case 82:
		b_82 = (*[82]byte)(b)
		goto sz_82
	case 83:
		b_83 = (*[83]byte)(b)
		goto sz_83
	case 84:
		b_84 = (*[84]byte)(b)
		goto sz_84
	case 85:
		b_85 = (*[85]byte)(b)
		goto sz_85
	case 86:
		b_86 = (*[86]byte)(b)
		goto sz_86
	case 87:
		b_87 = (*[87]byte)(b)
		goto sz_87
	case 88:
		b_88 = (*[88]byte)(b)
		goto sz_88
	case 89:
		b_89 = (*[89]byte)(b)
		goto sz_89
	case 90:
		b_90 = (*[90]byte)(b)
		goto sz_90
	case 91:
		b_91 = (*[91]byte)(b)
		goto sz_91
	case 92:
		b_92 = (*[92]byte)(b)
		goto sz_92
	case 93:
		b_93 = (*[93]byte)(b)
		goto sz_93
	case 94:
		b_94 = (*[94]byte)(b)
		goto sz_94
	case 95:
		b_95 = (*[95]byte)(b)
		goto sz_95
	case 96:
		b_96 = (*[96]byte)(b)
		goto sz_96
	case 97:
		b_97 = (*[97]byte)(b)
		goto sz_97
	case 98:
		b_98 = (*[98]byte)(b)
		goto sz_98
	case 99:
		b_99 = (*[99]byte)(b)
		goto sz_99
	case 100:
		b_100 = (*[100]byte)(b)
		goto sz_100
	case 101:
		b_101 = (*[101]byte)(b)
		goto sz_101
	case 102:
		b_102 = (*[102]byte)(b)
		goto sz_102
	case 103:
		b_103 = (*[103]byte)(b)
		goto sz_103
	case 104:
		b_104 = (*[104]byte)(b)
		goto sz_104
	case 105:
		b_105 = (*[105]byte)(b)
		goto sz_105
	case 106:
		b_106 = (*[106]byte)(b)
		goto sz_106
	case 107:
		b_107 = (*[107]byte)(b)
		goto sz_107
	case 108:
		b_108 = (*[108]byte)(b)
		goto sz_108
	case 109:
		b_109 = (*[109]byte)(b)
		goto sz_109
	case 110:
		b_110 = (*[110]byte)(b)
		goto sz_110
	case 111:
		b_111 = (*[111]byte)(b)
		goto sz_111
	case 112:
		b_112 = (*[112]byte)(b)
		goto sz_112
	case 113:
		b_113 = (*[113]byte)(b)
		goto sz_113
	case 114:
		b_114 = (*[114]byte)(b)
		goto sz_114
	case 115:
		b_115 = (*[115]byte)(b)
		goto sz_115
	case 116:
		b_116 = (*[116]byte)(b)
		goto sz_116
	case 117:
		b_117 = (*[117]byte)(b)
		goto sz_117
	case 118:
		b_118 = (*[118]byte)(b)
		goto sz_118
	case 119:
		b_119 = (*[119]byte)(b)
		goto sz_119
	case 120:
		b_120 = (*[120]byte)(b)
		goto sz_120
	case 121:
		b_121 = (*[121]byte)(b)
		goto sz_121
	case 122:
		b_122 = (*[122]byte)(b)
		goto sz_122
	case 123:
		b_123 = (*[123]byte)(b)
		goto sz_123
	case 124:
		b_124 = (*[124]byte)(b)
		goto sz_124
	case 125:
		b_125 = (*[125]byte)(b)
		goto sz_125
	case 126:
		b_126 = (*[126]byte)(b)
		goto sz_126
	case 127:
		b_127 = (*[127]byte)(b)
		goto sz_127
	default:
		panic("unreachable; slide overflow")
	}

	// Theses are the main slides, they handle 32 bytes 4 × 8 bytes at a time using ILP.
sz_127:
	v1 = round(v1, u64(b_127[0:8]))
	v2 = round(v2, u64(b_127[8:16]))
	v3 = round(v3, u64(b_127[16:24]))
	v4 = round(v4, u64(b_127[24:32]))
	b_95 = (*[95]byte)(b_127[32:])

sz_95:
	v1 = round(v1, u64(b_95[0:8]))
	v2 = round(v2, u64(b_95[8:16]))
	v3 = round(v3, u64(b_95[16:24]))
	v4 = round(v4, u64(b_95[24:32]))
	b_63 = (*[63]byte)(b_95[32:])

sz_63:
	v1 = round(v1, u64(b_63[0:8]))
	v2 = round(v2, u64(b_63[8:16]))
	v3 = round(v3, u64(b_63[16:24]))
	v4 = round(v4, u64(b_63[24:32]))
	b_31 = (*[31]byte)(b_63[32:])

	h = rol1(v1) + rol7(v2) + rol12(v3) + rol18(v4)
	h = mergeRound(h, v1)
	h = mergeRound(h, v2)
	h = mergeRound(h, v3)
	h = mergeRound(h, v4)

sz_31:
	h += n
	goto sz_31l

sz_126:
	v1 = round(v1, u64(b_126[0:8]))
	v2 = round(v2, u64(b_126[8:16]))
	v3 = round(v3, u64(b_126[16:24]))
	v4 = round(v4, u64(b_126[24:32]))
	b_94 = (*[94]byte)(b_126[32:])

sz_94:
	v1 = round(v1, u64(b_94[0:8]))
	v2 = round(v2, u64(b_94[8:16]))
	v3 = round(v3, u64(b_94[16:24]))
	v4 = round(v4, u64(b_94[24:32]))
	b_62 = (*[62]byte)(b_94[32:])

sz_62:
	v1 = round(v1, u64(b_62[0:8]))
	v2 = round(v2, u64(b_62[8:16]))
	v3 = round(v3, u64(b_62[16:24]))
	v4 = round(v4, u64(b_62[24:32]))
	b_30 = (*[30]byte)(b_62[32:])

	h = rol1(v1) + rol7(v2) + rol12(v3) + rol18(v4)
	h = mergeRound(h, v1)
	h = mergeRound(h, v2)
	h = mergeRound(h, v3)
	h = mergeRound(h, v4)

sz_30:
	h += n
	goto sz_30l

sz_125:
	v1 = round(v1, u64(b_125[0:8]))
	v2 = round(v2, u64(b_125[8:16]))
	v3 = round(v3, u64(b_125[16:24]))
	v4 = round(v4, u64(b_125[24:32]))
	b_93 = (*[93]byte)(b_125[32:])

sz_93:
	v1 = round(v1, u64(b_93[0:8]))
	v2 = round(v2, u64(b_93[8:16]))
	v3 = round(v3, u64(b_93[16:24]))
	v4 = round(v4, u64(b_93[24:32]))
	b_61 = (*[61]byte)(b_93[32:])

sz_61:
	v1 = round(v1, u64(b_61[0:8]))
	v2 = round(v2, u64(b_61[8:16]))
	v3 = round(v3, u64(b_61[16:24]))
	v4 = round(v4, u64(b_61[24:32]))
	b_29 = (*[29]byte)(b_61[32:])

	h = rol1(v1) + rol7(v2) + rol12(v3) + rol18(v4)
	h = mergeRound(h, v1)
	h = mergeRound(h, v2)
	h = mergeRound(h, v3)
	h = mergeRound(h, v4)

sz_29:
	h += n
	goto sz_29l

sz_124:
	v1 = round(v1, u64(b_124[0:8]))
	v2 = round(v2, u64(b_124[8:16]))
	v3 = round(v3, u64(b_124[16:24]))
	v4 = round(v4, u64(b_124[24:32]))
	b_92 = (*[92]byte)(b_124[32:])

sz_92:
	v1 = round(v1, u64(b_92[0:8]))
	v2 = round(v2, u64(b_92[8:16]))
	v3 = round(v3, u64(b_92[16:24]))
	v4 = round(v4, u64(b_92[24:32]))
	b_60 = (*[60]byte)(b_92[32:])

sz_60:
	v1 = round(v1, u64(b_60[0:8]))
	v2 = round(v2, u64(b_60[8:16]))
	v3 = round(v3, u64(b_60[16:24]))
	v4 = round(v4, u64(b_60[24:32]))
	b_28 = (*[28]byte)(b_60[32:])

	h = rol1(v1) + rol7(v2) + rol12(v3) + rol18(v4)
	h = mergeRound(h, v1)
	h = mergeRound(h, v2)
	h = mergeRound(h, v3)
	h = mergeRound(h, v4)

sz_28:
	h += n
	goto sz_28l

sz_123:
	v1 = round(v1, u64(b_123[0:8]))
	v2 = round(v2, u64(b_123[8:16]))
	v3 = round(v3, u64(b_123[16:24]))
	v4 = round(v4, u64(b_123[24:32]))
	b_91 = (*[91]byte)(b_123[32:])

sz_91:
	v1 = round(v1, u64(b_91[0:8]))
	v2 = round(v2, u64(b_91[8:16]))
	v3 = round(v3, u64(b_91[16:24]))
	v4 = round(v4, u64(b_91[24:32]))
	b_59 = (*[59]byte)(b_91[32:])

sz_59:
	v1 = round(v1, u64(b_59[0:8]))
	v2 = round(v2, u64(b_59[8:16]))
	v3 = round(v3, u64(b_59[16:24]))
	v4 = round(v4, u64(b_59[24:32]))
	b_27 = (*[27]byte)(b_59[32:])

	h = rol1(v1) + rol7(v2) + rol12(v3) + rol18(v4)
	h = mergeRound(h, v1)
	h = mergeRound(h, v2)
	h = mergeRound(h, v3)
	h = mergeRound(h, v4)

sz_27:
	h += n
	goto sz_27l

sz_122:
	v1 = round(v1, u64(b_122[0:8]))
	v2 = round(v2, u64(b_122[8:16]))
	v3 = round(v3, u64(b_122[16:24]))
	v4 = round(v4, u64(b_122[24:32]))
	b_90 = (*[90]byte)(b_122[32:])

sz_90:
	v1 = round(v1, u64(b_90[0:8]))
	v2 = round(v2, u64(b_90[8:16]))
	v3 = round(v3, u64(b_90[16:24]))
	v4 = round(v4, u64(b_90[24:32]))
	b_58 = (*[58]byte)(b_90[32:])

sz_58:
	v1 = round(v1, u64(b_58[0:8]))
	v2 = round(v2, u64(b_58[8:16]))
	v3 = round(v3, u64(b_58[16:24]))
	v4 = round(v4, u64(b_58[24:32]))
	b_26 = (*[26]byte)(b_58[32:])

	h = rol1(v1) + rol7(v2) + rol12(v3) + rol18(v4)
	h = mergeRound(h, v1)
	h = mergeRound(h, v2)
	h = mergeRound(h, v3)
	h = mergeRound(h, v4)

sz_26:
	h += n
	goto sz_26l

sz_121:
	v1 = round(v1, u64(b_121[0:8]))
	v2 = round(v2, u64(b_121[8:16]))
	v3 = round(v3, u64(b_121[16:24]))
	v4 = round(v4, u64(b_121[24:32]))
	b_89 = (*[89]byte)(b_121[32:])

sz_89:
	v1 = round(v1, u64(b_89[0:8]))
	v2 = round(v2, u64(b_89[8:16]))
	v3 = round(v3, u64(b_89[16:24]))
	v4 = round(v4, u64(b_89[24:32]))
	b_57 = (*[57]byte)(b_89[32:])

sz_57:
	v1 = round(v1, u64(b_57[0:8]))
	v2 = round(v2, u64(b_57[8:16]))
	v3 = round(v3, u64(b_57[16:24]))
	v4 = round(v4, u64(b_57[24:32]))
	b_25 = (*[25]byte)(b_57[32:])

	h = rol1(v1) + rol7(v2) + rol12(v3) + rol18(v4)
	h = mergeRound(h, v1)
	h = mergeRound(h, v2)
	h = mergeRound(h, v3)
	h = mergeRound(h, v4)

sz_25:
	h += n
	goto sz_25l

sz_120:
	v1 = round(v1, u64(b_120[0:8]))
	v2 = round(v2, u64(b_120[8:16]))
	v3 = round(v3, u64(b_120[16:24]))
	v4 = round(v4, u64(b_120[24:32]))
	b_88 = (*[88]byte)(b_120[32:])

sz_88:
	v1 = round(v1, u64(b_88[0:8]))
	v2 = round(v2, u64(b_88[8:16]))
	v3 = round(v3, u64(b_88[16:24]))
	v4 = round(v4, u64(b_88[24:32]))
	b_56 = (*[56]byte)(b_88[32:])

sz_56:
	v1 = round(v1, u64(b_56[0:8]))
	v2 = round(v2, u64(b_56[8:16]))
	v3 = round(v3, u64(b_56[16:24]))
	v4 = round(v4, u64(b_56[24:32]))
	b_24 = (*[24]byte)(b_56[32:])

	h = rol1(v1) + rol7(v2) + rol12(v3) + rol18(v4)
	h = mergeRound(h, v1)
	h = mergeRound(h, v2)
	h = mergeRound(h, v3)
	h = mergeRound(h, v4)

sz_24:
	h += n
	goto sz_24l

sz_119:
	v1 = round(v1, u64(b_119[0:8]))
	v2 = round(v2, u64(b_119[8:16]))
	v3 = round(v3, u64(b_119[16:24]))
	v4 = round(v4, u64(b_119[24:32]))
	b_87 = (*[87]byte)(b_119[32:])

sz_87:
	v1 = round(v1, u64(b_87[0:8]))
	v2 = round(v2, u64(b_87[8:16]))
	v3 = round(v3, u64(b_87[16:24]))
	v4 = round(v4, u64(b_87[24:32]))
	b_55 = (*[55]byte)(b_87[32:])

sz_55:
	v1 = round(v1, u64(b_55[0:8]))
	v2 = round(v2, u64(b_55[8:16]))
	v3 = round(v3, u64(b_55[16:24]))
	v4 = round(v4, u64(b_55[24:32]))
	b_23 = (*[23]byte)(b_55[32:])

	h = rol1(v1) + rol7(v2) + rol12(v3) + rol18(v4)
	h = mergeRound(h, v1)
	h = mergeRound(h, v2)
	h = mergeRound(h, v3)
	h = mergeRound(h, v4)

sz_23:
	h += n
	goto sz_23l

sz_118:
	v1 = round(v1, u64(b_118[0:8]))
	v2 = round(v2, u64(b_118[8:16]))
	v3 = round(v3, u64(b_118[16:24]))
	v4 = round(v4, u64(b_118[24:32]))
	b_86 = (*[86]byte)(b_118[32:])

sz_86:
	v1 = round(v1, u64(b_86[0:8]))
	v2 = round(v2, u64(b_86[8:16]))
	v3 = round(v3, u64(b_86[16:24]))
	v4 = round(v4, u64(b_86[24:32]))
	b_54 = (*[54]byte)(b_86[32:])

sz_54:
	v1 = round(v1, u64(b_54[0:8]))
	v2 = round(v2, u64(b_54[8:16]))
	v3 = round(v3, u64(b_54[16:24]))
	v4 = round(v4, u64(b_54[24:32]))
	b_22 = (*[22]byte)(b_54[32:])

	h = rol1(v1) + rol7(v2) + rol12(v3) + rol18(v4)
	h = mergeRound(h, v1)
	h = mergeRound(h, v2)
	h = mergeRound(h, v3)
	h = mergeRound(h, v4)

sz_22:
	h += n
	goto sz_22l

sz_117:
	v1 = round(v1, u64(b_117[0:8]))
	v2 = round(v2, u64(b_117[8:16]))
	v3 = round(v3, u64(b_117[16:24]))
	v4 = round(v4, u64(b_117[24:32]))
	b_85 = (*[85]byte)(b_117[32:])

sz_85:
	v1 = round(v1, u64(b_85[0:8]))
	v2 = round(v2, u64(b_85[8:16]))
	v3 = round(v3, u64(b_85[16:24]))
	v4 = round(v4, u64(b_85[24:32]))
	b_53 = (*[53]byte)(b_85[32:])

sz_53:
	v1 = round(v1, u64(b_53[0:8]))
	v2 = round(v2, u64(b_53[8:16]))
	v3 = round(v3, u64(b_53[16:24]))
	v4 = round(v4, u64(b_53[24:32]))
	b_21 = (*[21]byte)(b_53[32:])

	h = rol1(v1) + rol7(v2) + rol12(v3) + rol18(v4)
	h = mergeRound(h, v1)
	h = mergeRound(h, v2)
	h = mergeRound(h, v3)
	h = mergeRound(h, v4)

sz_21:
	h += n
	goto sz_21l

sz_116:
	v1 = round(v1, u64(b_116[0:8]))
	v2 = round(v2, u64(b_116[8:16]))
	v3 = round(v3, u64(b_116[16:24]))
	v4 = round(v4, u64(b_116[24:32]))
	b_84 = (*[84]byte)(b_116[32:])

sz_84:
	v1 = round(v1, u64(b_84[0:8]))
	v2 = round(v2, u64(b_84[8:16]))
	v3 = round(v3, u64(b_84[16:24]))
	v4 = round(v4, u64(b_84[24:32]))
	b_52 = (*[52]byte)(b_84[32:])

sz_52:
	v1 = round(v1, u64(b_52[0:8]))
	v2 = round(v2, u64(b_52[8:16]))
	v3 = round(v3, u64(b_52[16:24]))
	v4 = round(v4, u64(b_52[24:32]))
	b_20 = (*[20]byte)(b_52[32:])

	h = rol1(v1) + rol7(v2) + rol12(v3) + rol18(v4)
	h = mergeRound(h, v1)
	h = mergeRound(h, v2)
	h = mergeRound(h, v3)
	h = mergeRound(h, v4)

sz_20:
	h += n
	goto sz_20l

sz_115:
	v1 = round(v1, u64(b_115[0:8]))
	v2 = round(v2, u64(b_115[8:16]))
	v3 = round(v3, u64(b_115[16:24]))
	v4 = round(v4, u64(b_115[24:32]))
	b_83 = (*[83]byte)(b_115[32:])

sz_83:
	v1 = round(v1, u64(b_83[0:8]))
	v2 = round(v2, u64(b_83[8:16]))
	v3 = round(v3, u64(b_83[16:24]))
	v4 = round(v4, u64(b_83[24:32]))
	b_51 = (*[51]byte)(b_83[32:])

sz_51:
	v1 = round(v1, u64(b_51[0:8]))
	v2 = round(v2, u64(b_51[8:16]))
	v3 = round(v3, u64(b_51[16:24]))
	v4 = round(v4, u64(b_51[24:32]))
	b_19 = (*[19]byte)(b_51[32:])

	h = rol1(v1) + rol7(v2) + rol12(v3) + rol18(v4)
	h = mergeRound(h, v1)
	h = mergeRound(h, v2)
	h = mergeRound(h, v3)
	h = mergeRound(h, v4)

sz_19:
	h += n
	goto sz_19l

sz_114:
	v1 = round(v1, u64(b_114[0:8]))
	v2 = round(v2, u64(b_114[8:16]))
	v3 = round(v3, u64(b_114[16:24]))
	v4 = round(v4, u64(b_114[24:32]))
	b_82 = (*[82]byte)(b_114[32:])

sz_82:
	v1 = round(v1, u64(b_82[0:8]))
	v2 = round(v2, u64(b_82[8:16]))
	v3 = round(v3, u64(b_82[16:24]))
	v4 = round(v4, u64(b_82[24:32]))
	b_50 = (*[50]byte)(b_82[32:])

sz_50:
	v1 = round(v1, u64(b_50[0:8]))
	v2 = round(v2, u64(b_50[8:16]))
	v3 = round(v3, u64(b_50[16:24]))
	v4 = round(v4, u64(b_50[24:32]))
	b_18 = (*[18]byte)(b_50[32:])

	h = rol1(v1) + rol7(v2) + rol12(v3) + rol18(v4)
	h = mergeRound(h, v1)
	h = mergeRound(h, v2)
	h = mergeRound(h, v3)
	h = mergeRound(h, v4)

sz_18:
	h += n
	goto sz_18l

sz_113:
	v1 = round(v1, u64(b_113[0:8]))
	v2 = round(v2, u64(b_113[8:16]))
	v3 = round(v3, u64(b_113[16:24]))
	v4 = round(v4, u64(b_113[24:32]))
	b_81 = (*[81]byte)(b_113[32:])

sz_81:
	v1 = round(v1, u64(b_81[0:8]))
	v2 = round(v2, u64(b_81[8:16]))
	v3 = round(v3, u64(b_81[16:24]))
	v4 = round(v4, u64(b_81[24:32]))
	b_49 = (*[49]byte)(b_81[32:])

sz_49:
	v1 = round(v1, u64(b_49[0:8]))
	v2 = round(v2, u64(b_49[8:16]))
	v3 = round(v3, u64(b_49[16:24]))
	v4 = round(v4, u64(b_49[24:32]))
	b_17 = (*[17]byte)(b_49[32:])

	h = rol1(v1) + rol7(v2) + rol12(v3) + rol18(v4)
	h = mergeRound(h, v1)
	h = mergeRound(h, v2)
	h = mergeRound(h, v3)
	h = mergeRound(h, v4)

sz_17:
	h += n
	goto sz_17l

sz_112:
	v1 = round(v1, u64(b_112[0:8]))
	v2 = round(v2, u64(b_112[8:16]))
	v3 = round(v3, u64(b_112[16:24]))
	v4 = round(v4, u64(b_112[24:32]))
	b_80 = (*[80]byte)(b_112[32:])

sz_80:
	v1 = round(v1, u64(b_80[0:8]))
	v2 = round(v2, u64(b_80[8:16]))
	v3 = round(v3, u64(b_80[16:24]))
	v4 = round(v4, u64(b_80[24:32]))
	b_48 = (*[48]byte)(b_80[32:])

sz_48:
	v1 = round(v1, u64(b_48[0:8]))
	v2 = round(v2, u64(b_48[8:16]))
	v3 = round(v3, u64(b_48[16:24]))
	v4 = round(v4, u64(b_48[24:32]))
	b_16 = (*[16]byte)(b_48[32:])

	h = rol1(v1) + rol7(v2) + rol12(v3) + rol18(v4)
	h = mergeRound(h, v1)
	h = mergeRound(h, v2)
	h = mergeRound(h, v3)
	h = mergeRound(h, v4)

sz_16:
	h += n
	goto sz_16l

sz_111:
	v1 = round(v1, u64(b_111[0:8]))
	v2 = round(v2, u64(b_111[8:16]))
	v3 = round(v3, u64(b_111[16:24]))
	v4 = round(v4, u64(b_111[24:32]))
	b_79 = (*[79]byte)(b_111[32:])

sz_79:
	v1 = round(v1, u64(b_79[0:8]))
	v2 = round(v2, u64(b_79[8:16]))
	v3 = round(v3, u64(b_79[16:24]))
	v4 = round(v4, u64(b_79[24:32]))
	b_47 = (*[47]byte)(b_79[32:])

sz_47:
	v1 = round(v1, u64(b_47[0:8]))
	v2 = round(v2, u64(b_47[8:16]))
	v3 = round(v3, u64(b_47[16:24]))
	v4 = round(v4, u64(b_47[24:32]))
	b_15 = (*[15]byte)(b_47[32:])

	h = rol1(v1) + rol7(v2) + rol12(v3) + rol18(v4)
	h = mergeRound(h, v1)
	h = mergeRound(h, v2)
	h = mergeRound(h, v3)
	h = mergeRound(h, v4)

sz_15:
	h += n
	goto sz_15l

sz_110:
	v1 = round(v1, u64(b_110[0:8]))
	v2 = round(v2, u64(b_110[8:16]))
	v3 = round(v3, u64(b_110[16:24]))
	v4 = round(v4, u64(b_110[24:32]))
	b_78 = (*[78]byte)(b_110[32:])

sz_78:
	v1 = round(v1, u64(b_78[0:8]))
	v2 = round(v2, u64(b_78[8:16]))
	v3 = round(v3, u64(b_78[16:24]))
	v4 = round(v4, u64(b_78[24:32]))
	b_46 = (*[46]byte)(b_78[32:])

sz_46:
	v1 = round(v1, u64(b_46[0:8]))
	v2 = round(v2, u64(b_46[8:16]))
	v3 = round(v3, u64(b_46[16:24]))
	v4 = round(v4, u64(b_46[24:32]))
	b_14 = (*[14]byte)(b_46[32:])

	h = rol1(v1) + rol7(v2) + rol12(v3) + rol18(v4)
	h = mergeRound(h, v1)
	h = mergeRound(h, v2)
	h = mergeRound(h, v3)
	h = mergeRound(h, v4)

sz_14:
	h += n
	goto sz_14l

sz_109:
	v1 = round(v1, u64(b_109[0:8]))
	v2 = round(v2, u64(b_109[8:16]))
	v3 = round(v3, u64(b_109[16:24]))
	v4 = round(v4, u64(b_109[24:32]))
	b_77 = (*[77]byte)(b_109[32:])

sz_77:
	v1 = round(v1, u64(b_77[0:8]))
	v2 = round(v2, u64(b_77[8:16]))
	v3 = round(v3, u64(b_77[16:24]))
	v4 = round(v4, u64(b_77[24:32]))
	b_45 = (*[45]byte)(b_77[32:])

sz_45:
	v1 = round(v1, u64(b_45[0:8]))
	v2 = round(v2, u64(b_45[8:16]))
	v3 = round(v3, u64(b_45[16:24]))
	v4 = round(v4, u64(b_45[24:32]))
	b_13 = (*[13]byte)(b_45[32:])

	h = rol1(v1) + rol7(v2) + rol12(v3) + rol18(v4)
	h = mergeRound(h, v1)
	h = mergeRound(h, v2)
	h = mergeRound(h, v3)
	h = mergeRound(h, v4)

sz_13:
	h += n
	goto sz_13l

sz_108:
	v1 = round(v1, u64(b_108[0:8]))
	v2 = round(v2, u64(b_108[8:16]))
	v3 = round(v3, u64(b_108[16:24]))
	v4 = round(v4, u64(b_108[24:32]))
	b_76 = (*[76]byte)(b_108[32:])

sz_76:
	v1 = round(v1, u64(b_76[0:8]))
	v2 = round(v2, u64(b_76[8:16]))
	v3 = round(v3, u64(b_76[16:24]))
	v4 = round(v4, u64(b_76[24:32]))
	b_44 = (*[44]byte)(b_76[32:])

sz_44:
	v1 = round(v1, u64(b_44[0:8]))
	v2 = round(v2, u64(b_44[8:16]))
	v3 = round(v3, u64(b_44[16:24]))
	v4 = round(v4, u64(b_44[24:32]))
	b_12 = (*[12]byte)(b_44[32:])

	h = rol1(v1) + rol7(v2) + rol12(v3) + rol18(v4)
	h = mergeRound(h, v1)
	h = mergeRound(h, v2)
	h = mergeRound(h, v3)
	h = mergeRound(h, v4)

sz_12:
	h += n
	goto sz_12l

sz_107:
	v1 = round(v1, u64(b_107[0:8]))
	v2 = round(v2, u64(b_107[8:16]))
	v3 = round(v3, u64(b_107[16:24]))
	v4 = round(v4, u64(b_107[24:32]))
	b_75 = (*[75]byte)(b_107[32:])

sz_75:
	v1 = round(v1, u64(b_75[0:8]))
	v2 = round(v2, u64(b_75[8:16]))
	v3 = round(v3, u64(b_75[16:24]))
	v4 = round(v4, u64(b_75[24:32]))
	b_43 = (*[43]byte)(b_75[32:])

sz_43:
	v1 = round(v1, u64(b_43[0:8]))
	v2 = round(v2, u64(b_43[8:16]))
	v3 = round(v3, u64(b_43[16:24]))
	v4 = round(v4, u64(b_43[24:32]))
	b_11 = (*[11]byte)(b_43[32:])

	h = rol1(v1) + rol7(v2) + rol12(v3) + rol18(v4)
	h = mergeRound(h, v1)
	h = mergeRound(h, v2)
	h = mergeRound(h, v3)
	h = mergeRound(h, v4)

sz_11:
	h += n
	goto sz_11l

sz_106:
	v1 = round(v1, u64(b_106[0:8]))
	v2 = round(v2, u64(b_106[8:16]))
	v3 = round(v3, u64(b_106[16:24]))
	v4 = round(v4, u64(b_106[24:32]))
	b_74 = (*[74]byte)(b_106[32:])

sz_74:
	v1 = round(v1, u64(b_74[0:8]))
	v2 = round(v2, u64(b_74[8:16]))
	v3 = round(v3, u64(b_74[16:24]))
	v4 = round(v4, u64(b_74[24:32]))
	b_42 = (*[42]byte)(b_74[32:])

sz_42:
	v1 = round(v1, u64(b_42[0:8]))
	v2 = round(v2, u64(b_42[8:16]))
	v3 = round(v3, u64(b_42[16:24]))
	v4 = round(v4, u64(b_42[24:32]))
	b_10 = (*[10]byte)(b_42[32:])

	h = rol1(v1) + rol7(v2) + rol12(v3) + rol18(v4)
	h = mergeRound(h, v1)
	h = mergeRound(h, v2)
	h = mergeRound(h, v3)
	h = mergeRound(h, v4)

sz_10:
	h += n
	goto sz_10l

sz_105:
	v1 = round(v1, u64(b_105[0:8]))
	v2 = round(v2, u64(b_105[8:16]))
	v3 = round(v3, u64(b_105[16:24]))
	v4 = round(v4, u64(b_105[24:32]))
	b_73 = (*[73]byte)(b_105[32:])

sz_73:
	v1 = round(v1, u64(b_73[0:8]))
	v2 = round(v2, u64(b_73[8:16]))
	v3 = round(v3, u64(b_73[16:24]))
	v4 = round(v4, u64(b_73[24:32]))
	b_41 = (*[41]byte)(b_73[32:])

sz_41:
	v1 = round(v1, u64(b_41[0:8]))
	v2 = round(v2, u64(b_41[8:16]))
	v3 = round(v3, u64(b_41[16:24]))
	v4 = round(v4, u64(b_41[24:32]))
	b_9 = (*[9]byte)(b_41[32:])

	h = rol1(v1) + rol7(v2) + rol12(v3) + rol18(v4)
	h = mergeRound(h, v1)
	h = mergeRound(h, v2)
	h = mergeRound(h, v3)
	h = mergeRound(h, v4)

sz_9:
	h += n
	goto sz_9l

sz_104:
	v1 = round(v1, u64(b_104[0:8]))
	v2 = round(v2, u64(b_104[8:16]))
	v3 = round(v3, u64(b_104[16:24]))
	v4 = round(v4, u64(b_104[24:32]))
	b_72 = (*[72]byte)(b_104[32:])

sz_72:
	v1 = round(v1, u64(b_72[0:8]))
	v2 = round(v2, u64(b_72[8:16]))
	v3 = round(v3, u64(b_72[16:24]))
	v4 = round(v4, u64(b_72[24:32]))
	b_40 = (*[40]byte)(b_72[32:])

sz_40:
	v1 = round(v1, u64(b_40[0:8]))
	v2 = round(v2, u64(b_40[8:16]))
	v3 = round(v3, u64(b_40[16:24]))
	v4 = round(v4, u64(b_40[24:32]))
	b_8 = (*[8]byte)(b_40[32:])

	h = rol1(v1) + rol7(v2) + rol12(v3) + rol18(v4)
	h = mergeRound(h, v1)
	h = mergeRound(h, v2)
	h = mergeRound(h, v3)
	h = mergeRound(h, v4)

sz_8:
	h += n
	goto sz_8l

sz_103:
	v1 = round(v1, u64(b_103[0:8]))
	v2 = round(v2, u64(b_103[8:16]))
	v3 = round(v3, u64(b_103[16:24]))
	v4 = round(v4, u64(b_103[24:32]))
	b_71 = (*[71]byte)(b_103[32:])

sz_71:
	v1 = round(v1, u64(b_71[0:8]))
	v2 = round(v2, u64(b_71[8:16]))
	v3 = round(v3, u64(b_71[16:24]))
	v4 = round(v4, u64(b_71[24:32]))
	b_39 = (*[39]byte)(b_71[32:])

sz_39:
	v1 = round(v1, u64(b_39[0:8]))
	v2 = round(v2, u64(b_39[8:16]))
	v3 = round(v3, u64(b_39[16:24]))
	v4 = round(v4, u64(b_39[24:32]))
	b_7 = (*[7]byte)(b_39[32:])

	h = rol1(v1) + rol7(v2) + rol12(v3) + rol18(v4)
	h = mergeRound(h, v1)
	h = mergeRound(h, v2)
	h = mergeRound(h, v3)
	h = mergeRound(h, v4)

sz_7:
	h += n
	goto sz_7l

sz_102:
	v1 = round(v1, u64(b_102[0:8]))
	v2 = round(v2, u64(b_102[8:16]))
	v3 = round(v3, u64(b_102[16:24]))
	v4 = round(v4, u64(b_102[24:32]))
	b_70 = (*[70]byte)(b_102[32:])

sz_70:
	v1 = round(v1, u64(b_70[0:8]))
	v2 = round(v2, u64(b_70[8:16]))
	v3 = round(v3, u64(b_70[16:24]))
	v4 = round(v4, u64(b_70[24:32]))
	b_38 = (*[38]byte)(b_70[32:])

sz_38:
	v1 = round(v1, u64(b_38[0:8]))
	v2 = round(v2, u64(b_38[8:16]))
	v3 = round(v3, u64(b_38[16:24]))
	v4 = round(v4, u64(b_38[24:32]))
	b_6 = (*[6]byte)(b_38[32:])

	h = rol1(v1) + rol7(v2) + rol12(v3) + rol18(v4)
	h = mergeRound(h, v1)
	h = mergeRound(h, v2)
	h = mergeRound(h, v3)
	h = mergeRound(h, v4)

sz_6:
	h += n
	goto sz_6l

sz_101:
	v1 = round(v1, u64(b_101[0:8]))
	v2 = round(v2, u64(b_101[8:16]))
	v3 = round(v3, u64(b_101[16:24]))
	v4 = round(v4, u64(b_101[24:32]))
	b_69 = (*[69]byte)(b_101[32:])

sz_69:
	v1 = round(v1, u64(b_69[0:8]))
	v2 = round(v2, u64(b_69[8:16]))
	v3 = round(v3, u64(b_69[16:24]))
	v4 = round(v4, u64(b_69[24:32]))
	b_37 = (*[37]byte)(b_69[32:])

sz_37:
	v1 = round(v1, u64(b_37[0:8]))
	v2 = round(v2, u64(b_37[8:16]))
	v3 = round(v3, u64(b_37[16:24]))
	v4 = round(v4, u64(b_37[24:32]))
	b_5 = (*[5]byte)(b_37[32:])

	h = rol1(v1) + rol7(v2) + rol12(v3) + rol18(v4)
	h = mergeRound(h, v1)
	h = mergeRound(h, v2)
	h = mergeRound(h, v3)
	h = mergeRound(h, v4)

sz_5:
	h += n
	goto sz_5l

sz_100:
	v1 = round(v1, u64(b_100[0:8]))
	v2 = round(v2, u64(b_100[8:16]))
	v3 = round(v3, u64(b_100[16:24]))
	v4 = round(v4, u64(b_100[24:32]))
	b_68 = (*[68]byte)(b_100[32:])

sz_68:
	v1 = round(v1, u64(b_68[0:8]))
	v2 = round(v2, u64(b_68[8:16]))
	v3 = round(v3, u64(b_68[16:24]))
	v4 = round(v4, u64(b_68[24:32]))
	b_36 = (*[36]byte)(b_68[32:])

sz_36:
	v1 = round(v1, u64(b_36[0:8]))
	v2 = round(v2, u64(b_36[8:16]))
	v3 = round(v3, u64(b_36[16:24]))
	v4 = round(v4, u64(b_36[24:32]))
	b_4 = (*[4]byte)(b_36[32:])

	h = rol1(v1) + rol7(v2) + rol12(v3) + rol18(v4)
	h = mergeRound(h, v1)
	h = mergeRound(h, v2)
	h = mergeRound(h, v3)
	h = mergeRound(h, v4)

sz_4:
	h += n
	goto sz_4l

sz_99:
	v1 = round(v1, u64(b_99[0:8]))
	v2 = round(v2, u64(b_99[8:16]))
	v3 = round(v3, u64(b_99[16:24]))
	v4 = round(v4, u64(b_99[24:32]))
	b_67 = (*[67]byte)(b_99[32:])

sz_67:
	v1 = round(v1, u64(b_67[0:8]))
	v2 = round(v2, u64(b_67[8:16]))
	v3 = round(v3, u64(b_67[16:24]))
	v4 = round(v4, u64(b_67[24:32]))
	b_35 = (*[35]byte)(b_67[32:])

sz_35:
	v1 = round(v1, u64(b_35[0:8]))
	v2 = round(v2, u64(b_35[8:16]))
	v3 = round(v3, u64(b_35[16:24]))
	v4 = round(v4, u64(b_35[24:32]))
	b_3 = (*[3]byte)(b_35[32:])

	h = rol1(v1) + rol7(v2) + rol12(v3) + rol18(v4)
	h = mergeRound(h, v1)
	h = mergeRound(h, v2)
	h = mergeRound(h, v3)
	h = mergeRound(h, v4)

sz_3:
	h += n
	goto sz_3l

sz_98:
	v1 = round(v1, u64(b_98[0:8]))
	v2 = round(v2, u64(b_98[8:16]))
	v3 = round(v3, u64(b_98[16:24]))
	v4 = round(v4, u64(b_98[24:32]))
	b_66 = (*[66]byte)(b_98[32:])

sz_66:
	v1 = round(v1, u64(b_66[0:8]))
	v2 = round(v2, u64(b_66[8:16]))
	v3 = round(v3, u64(b_66[16:24]))
	v4 = round(v4, u64(b_66[24:32]))
	b_34 = (*[34]byte)(b_66[32:])

sz_34:
	v1 = round(v1, u64(b_34[0:8]))
	v2 = round(v2, u64(b_34[8:16]))
	v3 = round(v3, u64(b_34[16:24]))
	v4 = round(v4, u64(b_34[24:32]))
	b_2 = (*[2]byte)(b_34[32:])

	h = rol1(v1) + rol7(v2) + rol12(v3) + rol18(v4)
	h = mergeRound(h, v1)
	h = mergeRound(h, v2)
	h = mergeRound(h, v3)
	h = mergeRound(h, v4)

sz_2:
	h += n
	goto sz_2l

sz_97:
	v1 = round(v1, u64(b_97[0:8]))
	v2 = round(v2, u64(b_97[8:16]))
	v3 = round(v3, u64(b_97[16:24]))
	v4 = round(v4, u64(b_97[24:32]))
	b_65 = (*[65]byte)(b_97[32:])

sz_65:
	v1 = round(v1, u64(b_65[0:8]))
	v2 = round(v2, u64(b_65[8:16]))
	v3 = round(v3, u64(b_65[16:24]))
	v4 = round(v4, u64(b_65[24:32]))
	b_33 = (*[33]byte)(b_65[32:])

sz_33:
	v1 = round(v1, u64(b_33[0:8]))
	v2 = round(v2, u64(b_33[8:16]))
	v3 = round(v3, u64(b_33[16:24]))
	v4 = round(v4, u64(b_33[24:32]))
	b_1 = (*[1]byte)(b_33[32:])

	h = rol1(v1) + rol7(v2) + rol12(v3) + rol18(v4)
	h = mergeRound(h, v1)
	h = mergeRound(h, v2)
	h = mergeRound(h, v3)
	h = mergeRound(h, v4)

sz_1:
	h += n
	goto sz_1l

sz_96:
	v1 = round(v1, u64(b_96[0:8]))
	v2 = round(v2, u64(b_96[8:16]))
	v3 = round(v3, u64(b_96[16:24]))
	v4 = round(v4, u64(b_96[24:32]))
	b_64 = (*[64]byte)(b_96[32:])

sz_64:
	v1 = round(v1, u64(b_64[0:8]))
	v2 = round(v2, u64(b_64[8:16]))
	v3 = round(v3, u64(b_64[16:24]))
	v4 = round(v4, u64(b_64[24:32]))
	b_32 = (*[32]byte)(b_64[32:])

sz_32:
	v1 = round(v1, u64(b_32[0:8]))
	v2 = round(v2, u64(b_32[8:16]))
	v3 = round(v3, u64(b_32[16:24]))
	v4 = round(v4, u64(b_32[24:32]))
	b_0 = (*[0]byte)(b_32[32:])

	h = rol1(v1) + rol7(v2) + rol12(v3) + rol18(v4)
	h = mergeRound(h, v1)
	h = mergeRound(h, v2)
	h = mergeRound(h, v3)
	h = mergeRound(h, v4)

	h += n
	goto sz_0l

	// Theses are 8 bytes block trailing slides.
sz_31l:
	h ^= round(0, u64(b_31[:8]))
	h = rol27(h)*prime1 + prime4
	b_23 = (*[23]byte)(b_31[8:])

sz_23l:
	h ^= round(0, u64(b_23[:8]))
	h = rol27(h)*prime1 + prime4
	b_15 = (*[15]byte)(b_23[8:])

sz_15l:
	h ^= round(0, u64(b_15[:8]))
	h = rol27(h)*prime1 + prime4
	b_7 = (*[7]byte)(b_15[8:])

goto sz_7l

sz_30l:
	h ^= round(0, u64(b_30[:8]))
	h = rol27(h)*prime1 + prime4
	b_22 = (*[22]byte)(b_30[8:])

sz_22l:
	h ^= round(0, u64(b_22[:8]))
	h = rol27(h)*prime1 + prime4
	b_14 = (*[14]byte)(b_22[8:])

sz_14l:
	h ^= round(0, u64(b_14[:8]))
	h = rol27(h)*prime1 + prime4
	b_6 = (*[6]byte)(b_14[8:])

goto sz_6l

sz_29l:
	h ^= round(0, u64(b_29[:8]))
	h = rol27(h)*prime1 + prime4
	b_21 = (*[21]byte)(b_29[8:])

sz_21l:
	h ^= round(0, u64(b_21[:8]))
	h = rol27(h)*prime1 + prime4
	b_13 = (*[13]byte)(b_21[8:])

sz_13l:
	h ^= round(0, u64(b_13[:8]))
	h = rol27(h)*prime1 + prime4
	b_5 = (*[5]byte)(b_13[8:])

goto sz_5l

sz_28l:
	h ^= round(0, u64(b_28[:8]))
	h = rol27(h)*prime1 + prime4
	b_20 = (*[20]byte)(b_28[8:])

sz_20l:
	h ^= round(0, u64(b_20[:8]))
	h = rol27(h)*prime1 + prime4
	b_12 = (*[12]byte)(b_20[8:])

sz_12l:
	h ^= round(0, u64(b_12[:8]))
	h = rol27(h)*prime1 + prime4
	b_4 = (*[4]byte)(b_12[8:])

goto sz_4l

sz_27l:
	h ^= round(0, u64(b_27[:8]))
	h = rol27(h)*prime1 + prime4
	b_19 = (*[19]byte)(b_27[8:])

sz_19l:
	h ^= round(0, u64(b_19[:8]))
	h = rol27(h)*prime1 + prime4
	b_11 = (*[11]byte)(b_19[8:])

sz_11l:
	h ^= round(0, u64(b_11[:8]))
	h = rol27(h)*prime1 + prime4
	b_3 = (*[3]byte)(b_11[8:])

goto sz_3l

sz_26l:
	h ^= round(0, u64(b_26[:8]))
	h = rol27(h)*prime1 + prime4
	b_18 = (*[18]byte)(b_26[8:])

sz_18l:
	h ^= round(0, u64(b_18[:8]))
	h = rol27(h)*prime1 + prime4
	b_10 = (*[10]byte)(b_18[8:])

sz_10l:
	h ^= round(0, u64(b_10[:8]))
	h = rol27(h)*prime1 + prime4
	b_2 = (*[2]byte)(b_10[8:])

goto sz_2l

sz_25l:
	h ^= round(0, u64(b_25[:8]))
	h = rol27(h)*prime1 + prime4
	b_17 = (*[17]byte)(b_25[8:])

sz_17l:
	h ^= round(0, u64(b_17[:8]))
	h = rol27(h)*prime1 + prime4
	b_9 = (*[9]byte)(b_17[8:])

sz_9l:
	h ^= round(0, u64(b_9[:8]))
	h = rol27(h)*prime1 + prime4
	b_1 = (*[1]byte)(b_9[8:])

goto sz_1l

sz_24l:
	h ^= round(0, u64(b_24[:8]))
	h = rol27(h)*prime1 + prime4
	b_16 = (*[16]byte)(b_24[8:])

sz_16l:
	h ^= round(0, u64(b_16[:8]))
	h = rol27(h)*prime1 + prime4
	b_8 = (*[8]byte)(b_16[8:])

sz_8l:
	h ^= round(0, u64(b_8[:8]))
	h = rol27(h)*prime1 + prime4
	b_0 = (*[0]byte)(b_8[8:])

goto sz_0l

	// Theses are the 4 bytes trailing slides.
sz_7l:
	h ^= uint64(u32(b_7[:4])) * prime1
	h = rol23(h)*prime2 + prime3
	b_3 = (*[3]byte)(b_7[4:])
	goto sz_3l

sz_6l:
	h ^= uint64(u32(b_6[:4])) * prime1
	h = rol23(h)*prime2 + prime3
	b_2 = (*[2]byte)(b_6[4:])
	goto sz_2l

sz_5l:
	h ^= uint64(u32(b_5[:4])) * prime1
	h = rol23(h)*prime2 + prime3
	b_1 = (*[1]byte)(b_5[4:])
	goto sz_1l

sz_4l:
	h ^= uint64(u32(b_4[:4])) * prime1
	h = rol23(h)*prime2 + prime3
	b_0 = (*[0]byte)(b_4[4:])
	goto sz_0l

	// This is the 1 bytes trailing slide.
sz_3l:
	h ^= uint64(b_3[0]) * prime5
	h = rol11(h) * prime1
	b_2 = (*[2]byte)(b_3[1:])

sz_2l:
	h ^= uint64(b_2[0]) * prime5
	h = rol11(h) * prime1
	b_1 = (*[1]byte)(b_2[1:])

sz_1l:
	h ^= uint64(b_1[0]) * prime5
	h = rol11(h) * prime1
	b_0 = (*[0]byte)(b_1[1:])

	// Finally the terminator.
sz_0l:
	_ = b_0 // this avoids a bunch of if i != 0 { in codegen and is optimized away.

	h ^= h >> 33
	h *= prime2
	h ^= h >> 29
	h *= prime3
	h ^= h >> 32

	return h
}
